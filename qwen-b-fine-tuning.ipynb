{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12170339,"sourceType":"datasetVersion","datasetId":7665029}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:12:45.649304Z","iopub.execute_input":"2025-06-23T14:12:45.649532Z","iopub.status.idle":"2025-06-23T14:12:58.916146Z","shell.execute_reply.started":"2025-06-23T14:12:45.649507Z","shell.execute_reply":"2025-06-23T14:12:58.915047Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nfourbit_models = [\n    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/Phi-4\",\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-14B\",\n    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n    load_in_4bit = True,     # 4bit uses much less memory\n    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n    full_finetuning = False, # We have full finetuning now!\n    # token = \"hf_...\",      # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:12:58.918215Z","iopub.execute_input":"2025-06-23T14:12:58.918455Z","iopub.status.idle":"2025-06-23T14:14:32.913393Z","shell.execute_reply.started":"2025-06-23T14:12:58.918431Z","shell.execute_reply":"2025-06-23T14:14:32.912849Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-23 14:13:12.628741: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750687992.798167      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750687992.853537      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.6.5: Fast Qwen3 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/168k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83186373ee7b47c791346b5a763d2044"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795a5160cbd7438a870234e426bfacc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c94565775024af3b5e76cfcd6b638ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a5f8cf18814665983b7a12d6581d9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddbdc8eb0dbd4a1998d5a7a993197554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d73cf2c1f34464c8b052c1a30cbd1c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33fb1ebc1d7a4c99a97124ecde24c647"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18dc6e0939f404ea809e70cce2deb30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc92bcba613946af8b5d9c6c61365719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0ff72fa1b64235a2673042b4e6180b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27743a8f2d78479aa2855eb67bdcf6c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71fb8853d1d34a18b33dc72604ffdbfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/4.67k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4841a1c308ab4352a4485b8f0a148e80"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,   # We support rank stabilized LoRA\n    loftq_config = None,  # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:32.914096Z","iopub.execute_input":"2025-06-23T14:14:32.914703Z","iopub.status.idle":"2025-06-23T14:14:40.961596Z","shell.execute_reply.started":"2025-06-23T14:14:32.914673Z","shell.execute_reply":"2025-06-23T14:14:40.960982Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.5 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/intra-cuet-ml-contest/train.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:40.962298Z","iopub.execute_input":"2025-06-23T14:14:40.962573Z","iopub.status.idle":"2025-06-23T14:14:41.024644Z","shell.execute_reply.started":"2025-06-23T14:14:40.962551Z","shell.execute_reply":"2025-06-23T14:14:41.024028Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/intra-cuet-ml-contest/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.025281Z","iopub.execute_input":"2025-06-23T14:14:41.025481Z","iopub.status.idle":"2025-06-23T14:14:41.049357Z","shell.execute_reply.started":"2025-06-23T14:14:41.025464Z","shell.execute_reply":"2025-06-23T14:14:41.048838Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def generate_mcq_prompt(example):\n    question = example[\"question\"]\n    options = example[\"options\"]  # Assumes a list like [\"A. ...\", \"B. ...\", ...]\n    answer = example[\"answer\"]    # Assumes string like \"A\"\n\n    full_question = f\"{question}\\n\" + \"\\n\".join(options)\n    return {\n        \"conversations\": [\n            {\"role\": \"user\", \"content\": full_question},\n            {\"role\": \"assistant\", \"content\": answer}\n        ]\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.050196Z","iopub.execute_input":"2025-06-23T14:14:41.050511Z","iopub.status.idle":"2025-06-23T14:14:41.055093Z","shell.execute_reply.started":"2025-06-23T14:14:41.050466Z","shell.execute_reply":"2025-06-23T14:14:41.054402Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset\nhf_dataset = Dataset.from_pandas(df)\nprocessed_dataset = hf_dataset.map(generate_mcq_prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.056996Z","iopub.execute_input":"2025-06-23T14:14:41.057273Z","iopub.status.idle":"2025-06-23T14:14:41.240229Z","shell.execute_reply.started":"2025-06-23T14:14:41.057256Z","shell.execute_reply":"2025-06-23T14:14:41.239417Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b8b2a71c734e7ab3b065da4f6a9ec7"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"formatted_data = tokenizer.apply_chat_template(\n    processed_dataset[\"conversations\"],\n    tokenize = False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.240960Z","iopub.execute_input":"2025-06-23T14:14:41.241163Z","iopub.status.idle":"2025-06-23T14:14:41.392764Z","shell.execute_reply.started":"2025-06-23T14:14:41.241146Z","shell.execute_reply":"2025-06-23T14:14:41.391939Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_texts = pd.Series(formatted_data)\ntrain_texts.name = \"text\"\ncombined_dataset = Dataset.from_pandas(pd.DataFrame(train_texts))\ncombined_dataset = combined_dataset.shuffle(seed=3407)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.393747Z","iopub.execute_input":"2025-06-23T14:14:41.393979Z","iopub.status.idle":"2025-06-23T14:14:41.409236Z","shell.execute_reply.started":"2025-06-23T14:14:41.393962Z","shell.execute_reply":"2025-06-23T14:14:41.408536Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from unsloth import is_bfloat16_supported\nfrom trl import SFTTrainer, SFTConfig\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = combined_dataset,  # YOUR prepared dataset\n    eval_dataset = None,\n    args = SFTConfig(\n        dataset_text_field = \"text\",\n        per_device_train_batch_size = 2,\n        bf16 = is_bfloat16_supported(),\n        fp16 = not is_bfloat16_supported(),\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 30,  # Increase this if needed\n        learning_rate = 2e-4,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\",\n    ),\n)\n\ntrainer_stats = trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:14:41.410100Z","iopub.execute_input":"2025-06-23T14:14:41.410356Z","iopub.status.idle":"2025-06-23T14:27:40.110043Z","shell.execute_reply.started":"2025-06-23T14:14:41.410340Z","shell.execute_reply":"2025-06-23T14:27:40.109315Z"}},"outputs":[{"name":"stderr","text":"average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29db221e6c7443c3a2587e547eed7f50"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,500 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 128,450,560/14,000,000,000 (0.92% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 12:25, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.078400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.032200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.004800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.777300</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.500700</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.397500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.416700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.063300</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.026000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.885300</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.779500</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.842900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.902400</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.695100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.667600</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.646200</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.668500</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.654900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.660800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.583800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.594700</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.696800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.555600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.529200</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.552100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.766500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.661600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.542700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.603700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:27:40.111093Z","iopub.execute_input":"2025-06-23T14:27:40.111640Z","iopub.status.idle":"2025-06-23T14:40:29.756343Z","shell.execute_reply.started":"2025-06-23T14:27:40.111616Z","shell.execute_reply":"2025-06-23T14:40:29.755552Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,500 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 128,450,560/14,000,000,000 (0.92% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 12:25, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.585200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.652300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.526500</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.527400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.587500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.591100</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.545700</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.481800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.476200</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.435300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.563000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.584000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.424500</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.454000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.448200</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.483800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.484400</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.453300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.386100</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.421500</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.513500</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.490700</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.402800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.352000</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.380700</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.563500</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.484000</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.383900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.437700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"messages = [\n    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize = False,\n    add_generation_prompt = True,\n    enable_thinking = True,\n)\n\nfrom transformers import TextStreamer\n_ = model.generate(\n    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n    max_new_tokens = 1024,\n    temperature = 0.1, top_p = 0.95, top_k = 50,\n    streamer = TextStreamer(tokenizer, skip_prompt = True),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:40:29.757351Z","iopub.execute_input":"2025-06-23T14:40:29.757639Z","iopub.status.idle":"2025-06-23T14:40:30.969567Z","shell.execute_reply.started":"2025-06-23T14:40:29.757618Z","shell.execute_reply":"2025-06-23T14:40:30.968968Z"}},"outputs":[{"name":"stdout","text":"<think>\n\n</think>\n\nB<|im_end|>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\")\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T14:40:30.970436Z","iopub.execute_input":"2025-06-23T14:40:30.970657Z","iopub.status.idle":"2025-06-23T14:40:32.801881Z","shell.execute_reply.started":"2025-06-23T14:40:30.970641Z","shell.execute_reply":"2025-06-23T14:40:32.801266Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/vocab.json',\n 'lora_model/merges.txt',\n 'lora_model/added_tokens.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nbase_model_name = \"unsloth/Qwen3-14B\"  # Same base model you used for training\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"/kaggle/working/lora_model\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n    dtype = torch.float16,\n)\n\n# Apply the saved LoRA weights\nmodel.load_adapter(\"/kaggle/working/lora_model\")\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-23T14:56:31.598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf_test = pd.read_csv(\"/kaggle/input/your-dataset/test.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_mcq_prompt(row):\n    options = [\n        f\"A. {row['option_1']}\",\n        f\"B. {row['option_2']}\",\n        f\"C. {row['option_3']}\",\n        f\"D. {row['option_4']}\",\n    ]\n    full_question = f\"{row['question']}\\n\" + \"\\n\".join(options)\n    return [{\"role\": \"user\", \"content\": full_question}]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\nimport torch\n\npredictions = []\n\nfor idx, row in df_test.iterrows():\n    messages = format_mcq_prompt(row)\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n        enable_thinking=False\n    )\n\n    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=1,\n            temperature=0.0,\n            do_sample=False,\n        )\n    prediction = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:]).strip()\n    prediction = prediction[0] if prediction and prediction[0] in [\"A\", \"B\", \"C\", \"D\"] else \"A\"  # fallback\n    predictions.append(prediction)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": df_test[\"id\"],\n    \"answer\": predictions\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}